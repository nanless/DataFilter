#!/usr/bin/env python3
"""
æ”¹è¿›çš„å¤šGPUé•¿éŸ³é¢‘å¤„ç†å¯åŠ¨è„šæœ¬
è§£å†³æ˜¾å­˜æ³„æ¼å’Œè¿›ç¨‹ç®¡ç†é—®é¢˜
"""

import os
import sys
import logging
import argparse
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä½¿æ¨¡å—å¯¼å…¥æ­£å¸¸å·¥ä½œ
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
sys.path.insert(0, str(parent_dir))

# ç°åœ¨å¯ä»¥æ­£ç¡®å¯¼å…¥æ¨¡å—
from long_speech_filter.config import LongAudioProcessingConfig
from long_speech_filter.multi_gpu_processor import MultiGPULongAudioProcessor, MultiGPUConfig


def setup_logging(log_level: str = "INFO", log_file: str = None):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # åŸºç¡€é…ç½®
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # é…ç½®root logger
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format=log_format,
        handlers=[]
    )
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(getattr(logging, log_level.upper()))
    console_handler.setFormatter(logging.Formatter(log_format))
    
    # æ–‡ä»¶å¤„ç†å™¨
    if log_file:
        file_handler = logging.FileHandler(log_dir / log_file, encoding='utf-8')
        file_handler.setLevel(getattr(logging, log_level.upper()))
        file_handler.setFormatter(logging.Formatter(log_format))
        logging.getLogger().addHandler(file_handler)
    
    logging.getLogger().addHandler(console_handler)


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="æ”¹è¿›çš„å¤šGPUé•¿éŸ³é¢‘å¤„ç†å™¨ - è§£å†³æ˜¾å­˜ç®¡ç†é—®é¢˜",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤é…ç½®
  python run_improved_multi_gpu.py
  
  # è‡ªå®šä¹‰è¾“å…¥è¾“å‡ºç›®å½•
  python run_improved_multi_gpu.py --input /path/to/input --output /path/to/output
  
  # ä¸¥æ ¼é™åˆ¶æ˜¾å­˜ä½¿ç”¨
  python run_improved_multi_gpu.py --memory-fraction 0.6 --processes-per-gpu 1
  
  # ä½¿ç”¨ç‰¹å®šGPU
  python run_improved_multi_gpu.py --num-gpus 4 --max-concurrent 4
        """
    )
    
    # è¾“å…¥è¾“å‡ºé…ç½®
    parser.add_argument(
        '--input', type=str,
        default='/root/code/github_repos/DataCrawler/ximalaya_downloader/downloads_mossformer_enhanced',
        help='è¾“å…¥ç›®å½•è·¯å¾„'
    )
    parser.add_argument(
        '--output', type=str,
        default='/root/code/github_repos/DataCrawler/ximalaya_downloader/downloads_mossformer_enhanced_filtered_2',
        help='è¾“å‡ºç›®å½•è·¯å¾„'
    )
    
    # GPUé…ç½®
    parser.add_argument(
        '--num-gpus', type=int, default=-1,
        help='ä½¿ç”¨çš„GPUæ•°é‡ (-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰GPU)'
    )
    parser.add_argument(
        '--processes-per-gpu', type=int, default=2,
        help='æ¯ä¸ªGPUçš„æœ€å¤§è¿›ç¨‹æ•° (å»ºè®®è®¾ç½®ä¸º1é¿å…æ˜¾å­˜ç«äº‰)'
    )
    parser.add_argument(
        '--memory-fraction', type=float, default=0.6,
        help='æ¯ä¸ªGPUä½¿ç”¨çš„æ˜¾å­˜æ¯”ä¾‹ (0.1-0.9, å»ºè®®0.6-0.7)'
    )
    parser.add_argument(
        '--max-concurrent', type=int, default=16,
        help='æœ€å¤§å¹¶å‘æ–‡ä»¶æ•°'
    )
    
    # æ¨¡å‹é…ç½®
    parser.add_argument(
        '--whisper-model', type=str, default='large-v3',
        help='Whisperæ¨¡å‹åç§°'
    )
    parser.add_argument(
        '--model-cache-dir', type=str, default='/root/data/pretrained_models',
        help='æ¨¡å‹ç¼“å­˜ç›®å½•'
    )
    
    # è´¨é‡ç­›é€‰é…ç½®
    parser.add_argument(
        '--min-words', type=int, default=1,
        help='æœ€å°‘è¯æ•°è¦æ±‚'
    )
    parser.add_argument(
        '--distilmos-threshold', type=float, default=3.0,
        help='DistilMOSé˜ˆå€¼'
    )
    parser.add_argument(
        '--dnsmos-threshold', type=float, default=3.0,
        help='DNSMOSé˜ˆå€¼'  
    )
    parser.add_argument(
        '--dnsmospro-threshold', type=float, default=3.0,
        help='DNSMOSProé˜ˆå€¼'
    )
    
    # æ—¥å¿—é…ç½®
    parser.add_argument(
        '--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO', help='æ—¥å¿—çº§åˆ«'
    )
    parser.add_argument(
        '--log-file', type=str, 
        default='improved_multi_gpu_processing.log',
        help='æ—¥å¿—æ–‡ä»¶å'
    )
    
    # è°ƒè¯•é€‰é¡¹
    parser.add_argument(
        '--dry-run', action='store_true',
        help='åªæ˜¾ç¤ºé…ç½®ä¿¡æ¯ï¼Œä¸å®é™…å¤„ç†'
    )
    parser.add_argument(
        '--test-mode', action='store_true',
        help='æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰5ä¸ªæ–‡ä»¶'
    )
    
    # å¤„ç†é€‰é¡¹
    parser.add_argument(
        '--skip-processed', action='store_true', default=True,
        help='è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ (é»˜è®¤å¯ç”¨)'
    )
    parser.add_argument(
        '--no-skip-processed', action='store_true',
        help='ä¸è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ï¼Œå¤„ç†æ‰€æœ‰æ–‡ä»¶'
    )
    parser.add_argument(
        '--force-reprocess', action='store_true',
        help='å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Œå³ä½¿å·²å­˜åœ¨ç»“æœ'
    )
    
    return parser.parse_args()


def validate_arguments(args):
    """éªŒè¯å‚æ•°"""
    errors = []
    
    # éªŒè¯è·¯å¾„
    if not Path(args.input).exists():
        errors.append(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input}")
    
    # éªŒè¯æ˜¾å­˜æ¯”ä¾‹
    if not (0.1 <= args.memory_fraction <= 0.9):
        errors.append(f"æ˜¾å­˜æ¯”ä¾‹å¿…é¡»åœ¨0.1-0.9ä¹‹é—´: {args.memory_fraction}")
    
    # éªŒè¯è¿›ç¨‹æ•°
    if args.processes_per_gpu < 1 or args.processes_per_gpu > 2:
        errors.append(f"æ¯GPUè¿›ç¨‹æ•°å»ºè®®è®¾ç½®ä¸º1-2: {args.processes_per_gpu}")
    
    # éªŒè¯é˜ˆå€¼
    for threshold_name, threshold_value in [
        ('distilmos-threshold', args.distilmos_threshold),
        ('dnsmos-threshold', args.dnsmos_threshold), 
        ('dnsmospro-threshold', args.dnsmospro_threshold)
    ]:
        if not (1.0 <= threshold_value <= 5.0):
            errors.append(f"{threshold_name}å¿…é¡»åœ¨1.0-5.0ä¹‹é—´: {threshold_value}")
    
    if errors:
        print("âŒ å‚æ•°éªŒè¯å¤±è´¥:")
        for error in errors:
            print(f"   â€¢ {error}")
        sys.exit(1)


def create_configs(args):
    """åˆ›å»ºé…ç½®å¯¹è±¡"""
    # åŸºç¡€å¤„ç†é…ç½®
    base_config = LongAudioProcessingConfig()
    base_config.input_dir = args.input
    base_config.output_dir = args.output
    base_config.whisper.model_name = args.whisper_model
    base_config.whisper.model_cache_dir = args.model_cache_dir
    base_config.quality_filter.min_words = args.min_words
    base_config.quality_filter.distil_mos_threshold = args.distilmos_threshold
    base_config.quality_filter.dnsmos_threshold = args.dnsmos_threshold
    base_config.quality_filter.dnsmospro_threshold = args.dnsmospro_threshold
    base_config.log_level = args.log_level
    base_config.log_file = args.log_file
    
    # å¤„ç†è·³è¿‡å·²å¤„ç†æ–‡ä»¶çš„é€»è¾‘
    if args.force_reprocess:
        # å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰æ–‡ä»¶
        base_config.processing.skip_processed = False
        base_config.processing.force_reprocess = True
    elif args.no_skip_processed:
        # ä¸è·³è¿‡å·²å¤„ç†æ–‡ä»¶ï¼Œä½†ä¹Ÿä¸å¼ºåˆ¶é‡æ–°å¤„ç†
        base_config.processing.skip_processed = False
        base_config.processing.force_reprocess = False
    else:
        # é»˜è®¤è·³è¿‡å·²å¤„ç†æ–‡ä»¶
        base_config.processing.skip_processed = True
        base_config.processing.force_reprocess = False
    
    # å¤šGPUé…ç½®
    multi_gpu_config = MultiGPUConfig(
        num_gpus=args.num_gpus,
        max_processes_per_gpu=args.processes_per_gpu,
        gpu_memory_fraction=args.memory_fraction,
        max_concurrent_files=args.max_concurrent,
        enable_gpu_monitoring=True
    )
    
    return base_config, multi_gpu_config


def print_config_summary(base_config, multi_gpu_config):
    """æ‰“å°é…ç½®æ‘˜è¦"""
    print("=" * 80)
    print("ğŸ“‹ æ”¹è¿›çš„å¤šGPUé•¿éŸ³é¢‘å¤„ç†å™¨é…ç½®æ‘˜è¦")
    print("=" * 80)
    
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {base_config.input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {base_config.output_dir}")
    print()
    
    print("ğŸ–¥ï¸ GPUé…ç½®:")
    print(f"   â€¢ GPUæ•°é‡: {multi_gpu_config.num_gpus} (-1è¡¨ç¤ºå…¨éƒ¨)")
    print(f"   â€¢ æ¯GPUè¿›ç¨‹æ•°: {multi_gpu_config.max_processes_per_gpu}")
    print(f"   â€¢ æ˜¾å­˜ä½¿ç”¨æ¯”ä¾‹: {multi_gpu_config.gpu_memory_fraction:.1%}")
    print(f"   â€¢ æœ€å¤§å¹¶å‘æ–‡ä»¶æ•°: {multi_gpu_config.max_concurrent_files}")
    print()
    
    print("ğŸ¤ æ¨¡å‹é…ç½®:")
    print(f"   â€¢ Whisperæ¨¡å‹: {base_config.whisper.model_name}")
    print(f"   â€¢ æ¨¡å‹ç¼“å­˜ç›®å½•: {base_config.whisper.model_cache_dir}")
    print()
    
    print("ğŸ“Š è´¨é‡ç­›é€‰é˜ˆå€¼:")
    print(f"   â€¢ æœ€å°‘è¯æ•°: {base_config.quality_filter.min_words}")
    print(f"   â€¢ DistilMOS â‰¥ {base_config.quality_filter.distil_mos_threshold}")
    print(f"   â€¢ DNSMOS â‰¥ {base_config.quality_filter.dnsmos_threshold}")
    print(f"   â€¢ DNSMOSPro â‰¥ {base_config.quality_filter.dnsmospro_threshold}")
    print()
    
    print("ğŸ”„ å¤„ç†é€‰é¡¹:")
    if base_config.processing.force_reprocess:
        print("   â€¢ âš¡ å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰æ–‡ä»¶")
    elif base_config.processing.skip_processed:
        print("   â€¢ â­ï¸ è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ (é»˜è®¤)")
    else:
        print("   â€¢ ğŸ”„ å¤„ç†æ‰€æœ‰æ–‡ä»¶ (ä¸è·³è¿‡)")
    print()
    
    print("ğŸ”§ æ”¹è¿›ç‰¹æ€§:")
    print("   â€¢ âœ… ä¸¥æ ¼çš„æ¯GPUä¸€è¿›ç¨‹é™åˆ¶")
    print("   â€¢ âœ… ä¸»åŠ¨æ˜¾å­˜æ¸…ç†å’Œç›‘æ§")
    print("   â€¢ âœ… å¸¦é‡è¯•æœºåˆ¶çš„æ¨¡å‹åŠ è½½")
    print("   â€¢ âœ… æ‰¹é‡å¤„ç†æ˜¾å­˜ç®¡ç†")
    print("   â€¢ âœ… CPUåå¤‡æ¨¡å¼æ”¯æŒ")
    print("   â€¢ âœ… è¯¦ç»†çš„è¿›åº¦å’Œæ˜¾å­˜ç›‘æ§")
    print("   â€¢ âœ… æ™ºèƒ½è·³è¿‡å·²å¤„ç†æ–‡ä»¶")
    print("=" * 80)


def check_gpu_status():
    """æ£€æŸ¥GPUçŠ¶æ€"""
    try:
        import torch
        if not torch.cuda.is_available():
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨GPUåŠ é€Ÿ")
            return False
        
        num_gpus = torch.cuda.device_count()
        print(f"ğŸ–¥ï¸ æ£€æµ‹åˆ° {num_gpus} å¼ GPU:")
        
        for i in range(num_gpus):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / 1024**3
            
            # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            torch.cuda.set_device(i)
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            free = memory_gb - allocated
            
            print(f"   GPU {i}: {props.name}")
            print(f"      æ€»æ˜¾å­˜: {memory_gb:.1f}GB")
            print(f"      å·²ä½¿ç”¨: {allocated:.1f}GB ({allocated/memory_gb:.1%})")
            print(f"      å·²ç¼“å­˜: {cached:.1f}GB ({cached/memory_gb:.1%})")
            print(f"      å¯ç”¨: {free:.1f}GB ({free/memory_gb:.1%})")
            
            if free < 2.0:  # å¯ç”¨æ˜¾å­˜å°‘äº2GBæ—¶è­¦å‘Š
                print(f"      âš ï¸ è­¦å‘Š: å¯ç”¨æ˜¾å­˜ä¸è¶³ï¼Œå¯èƒ½å½±å“å¤„ç†")
        
        return True
        
    except Exception as e:
        print(f"âŒ GPUçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    args = parse_arguments()
    
    # éªŒè¯å‚æ•°
    validate_arguments(args)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level, args.log_file)
    
    # åˆ›å»ºé…ç½®
    base_config, multi_gpu_config = create_configs(args)
    
    # æ‰“å°é…ç½®æ‘˜è¦
    print_config_summary(base_config, multi_gpu_config)
    
    # æ£€æŸ¥GPUçŠ¶æ€
    if not check_gpu_status():
        print("âŒ GPUæ£€æŸ¥å¤±è´¥ï¼Œé€€å‡ºå¤„ç†")
        sys.exit(1)
    
    # å¹²è¿è¡Œæ¨¡å¼
    if args.dry_run:
        print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ï¼šé…ç½®éªŒè¯å®Œæˆï¼Œæœªæ‰§è¡Œå®é™…å¤„ç†")
        return
    
    print()
    print("ğŸš€ å¼€å§‹æ”¹è¿›çš„å¤šGPUå¹¶è¡Œå¤„ç†...")
    print()
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = MultiGPULongAudioProcessor(base_config, multi_gpu_config)
        
        # æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶æ–‡ä»¶æ•°é‡
        if args.test_mode:
            print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰5ä¸ªæ–‡ä»¶")
            # å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹é…ç½®æ¥é™åˆ¶æ–‡ä»¶æ•°é‡
        
        # æ‰§è¡Œå¤„ç†
        results = processor.process_directory_parallel()
        
        # è¾“å‡ºæœ€ç»ˆç»“æœ
        print()
        print("=" * 80)
        print("ğŸ‰ å¤„ç†å®Œæˆ!")
        print("=" * 80)
        print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {results['total_files']}")
        print(f"âœ… æˆåŠŸå¤„ç†: {results['successful_files']}")
        print(f"âŒ å¤±è´¥æ–‡ä»¶: {results['failed_files']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {results['success_rate']:.1f}%")
        print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {results['processing_time']/60:.1f}åˆ†é’Ÿ")
        print(f"âš¡ å¹³å‡å¤„ç†æ—¶é—´: {results['average_time_per_file']:.1f}ç§’/æ–‡ä»¶")
        
        # GPUä½¿ç”¨ç»Ÿè®¡
        print()
        print("ğŸ–¥ï¸ GPUä½¿ç”¨ç»Ÿè®¡:")
        for gpu_id, stats in results['gpu_stats'].items():
            memory_info = ""
            if 'memory_usage' in stats:
                memory_info = f", æœ€ç»ˆæ˜¾å­˜: {stats['memory_usage']:.1%}"
            print(f"   GPU {gpu_id}: å¤„ç†äº† {stats['processed_count']} ä¸ªæ–‡ä»¶{memory_info}")
        
        # æ˜¾å­˜ç»Ÿè®¡  
        if results.get('memory_stats', {}).get('peak_usage_by_gpu'):
            print()
            print("ğŸ“Š æ˜¾å­˜ä½¿ç”¨ç»Ÿè®¡:")
            for gpu_id, peak in results['memory_stats']['peak_usage_by_gpu'].items():
                avg = results['memory_stats']['average_usage_by_gpu'].get(gpu_id, 0)
                print(f"   GPU {gpu_id}: å³°å€¼ {peak:.1%}, å¹³å‡ {avg:.1%}")
        
        print("=" * 80)
        
        # è¿”å›æˆåŠŸçŠ¶æ€
        return 0 if results['successful_files'] > 0 else 1
        
    except KeyboardInterrupt:
        print()
        print("âš ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
        return 1
    except Exception as e:
        print()
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logging.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 